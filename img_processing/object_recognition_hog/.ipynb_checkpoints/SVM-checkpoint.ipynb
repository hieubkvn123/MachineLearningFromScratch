{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 153)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<tokenize>\"\u001b[0;36m, line \u001b[0;32m153\u001b[0m\n\u001b[0;31m    print(prediction)\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "# adaptive svm\n",
    "import math\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "# Note, this is for binary classification only\n",
    "def kernel(kernel='linear', gamma = 0.01, r=1/2, d=2):\n",
    "\t'''\n",
    "\t\tgamma : 1/2*sigma**2 for the gaussian kernel\n",
    "\t\tr : the bias for polynomial kernel\n",
    "\t\td : the power base for the polynomial kernel\n",
    "\t'''\n",
    "\n",
    "\t# the default linear kernel\n",
    "\toperation = lambda x, y : np.dot(x, y)\n",
    "\n",
    "\tif(kernel == 'rbf'):\n",
    "\t\toperation = lambda x, y : -gamma*np.linalg.norm(x-y)\n",
    "\tif(kernel == 'poly'):\n",
    "\t\toperation = lambda x, y : (np.dot(x,y) + r) ** d \n",
    "\n",
    "\treturn operation\n",
    "\n",
    "# mse loss\n",
    "def loss(predictions, labels):\n",
    "\tif(predictions.shape[0] != labels.shape[0]):\n",
    "\t\tprint(\"[INFO] Labels and Predictions are not of the same shape ... \")\n",
    "\t\treturn None\n",
    "\telse:\n",
    "\t\t# loop thru pairs of prediction and label\n",
    "\t\tloss = 0\n",
    "\t\tfor j in range(predictions.shape[0]): # or labels.shape[0]\n",
    "\t\t\tloss += (predictions[j] - labels[j]) ** 2\n",
    "\n",
    "\t\treturn math.sqrt(loss)\n",
    "\n",
    "class_1 = np.array([[2,3], [1,2], [2.5, 3.5],\n",
    "                    [1.5, 2.5], [2,2], [2.5,2.5],\n",
    "                    [1.5,3],[2.5,1],[1,1]])\n",
    "\n",
    "class_2 = np.array([[4,5],[5,5],[4,6],\n",
    "                    [4.5,5.5],[5.5,5.5],[6,6],\n",
    "                    [5,4],[5,6],[6,5]])\n",
    "\n",
    "# validation data\n",
    "class_1_ = np.array([[1.2,2.1],[2.1,2],[1.3,1.7]])\n",
    "class_2_ = np.array([[4,4],[3.8,5.4],[4.2,3.4]])\n",
    "\n",
    "x = np.concatenate((class_1, class_2))\n",
    "x_ = np.concatenate((class_1_, class_2_))\n",
    "y = np.array([1,1,1,1,1,1,1,1,1,-1,-1,-1,-1,-1,-1,-1,-1,-1])\n",
    "\n",
    "# let's go a little bit object oriented shall we\n",
    "class KernelSVM:\n",
    "\tdef __init__(self):\n",
    "\t\tself.x = None \n",
    "\t\tself.y = None \n",
    "\t\tself.w = None\n",
    "\t\tself.kernel = None\n",
    "\n",
    "\tdef fit(self,x, y, iterations=100000, alpha=0.001, l=0.01):\n",
    "\t\t'''\n",
    "\t\t\t- iterations is the max number of training iterations\n",
    "\t\t\t- alpha is the learning rate\n",
    "\t\t\t- l is the regularization parameter\n",
    "\t\t'''\n",
    "\n",
    "\t\t# just create a copy for inference purpose\n",
    "\t\tself.x = x\n",
    "\t\tself.y = y\n",
    "\n",
    "\t\tbias = 10.0\n",
    "\n",
    "\t\tif(not isinstance(x, np.ndarray) or not isinstance(y, np.ndarray)):\n",
    "\t\t\tprint(\"[INFO] The input and output must be numpy array ... \")\n",
    "\t\t\treturn None\n",
    "\t\telse:\n",
    "\t\t\t# check if the input is of right dimension\n",
    "\t\t\tif(len(x.shape) != 2 or len(y.shape) != 1):\n",
    "\t\t\t\tprint(\"[INFO] Input and output are of wrong shape ...\")\n",
    "\t\t\t\treturn None\n",
    "\t\t\telse:\n",
    "\t\t\t\t# svm is optimized based on hinge loss\n",
    "\t\t\t\t# hinge(x,w) = Max(0, 1 - y * <x,w>)\n",
    "\t\t\t\t# we need to optimize 2/||w|| -> the regularized loss is \n",
    "\t\t\t\t# L = summation(Max(0, 1 - y * <x,w>)) + ||w||**2/2\n",
    "\t\t\t\tK = kernel(kernel='rbf')\n",
    "\t\t\t\tself.kernel = K\n",
    "\n",
    "\t\t\t\t# first, calculate all f_i vectors from the kernel\n",
    "\t\t\t\tdataset_size = x.shape[0]\n",
    "\t\t\t\tf_i = np.zeros((dataset_size, dataset_size))\n",
    "\n",
    "\t\t\t\t# construct f_i which is a matrix of kernelized inputs\n",
    "\t\t\t\tfor i in range(dataset_size):\n",
    "\t\t\t\t\tf = np.zeros((dataset_size,))\n",
    "\t\t\t\t\tfor j in range(dataset_size):\n",
    "\t\t\t\t\t\tf[j] = K(x[i], x[j])\n",
    "\n",
    "\t\t\t\t\tf_i[i] = f \n",
    "\n",
    "\t\t\t\t# print(f_i)\n",
    "\n",
    "\t\t\t\t# now all we have to do is applying svm on f_1 and y\n",
    "\t\t\t\tself.w = np.ones((dataset_size, ), dtype=np.float32) # a vector with same size as f_i input vectors\n",
    "\n",
    "\t\t\t\t# loop thru the training iterations\n",
    "\t\t\t\tprevious_loss = 0\n",
    "\t\t\t\tfor i in range(iterations) :\n",
    "\t\t\t\t\tpredictions = np.zeros((y.shape[0], ))\n",
    "\t\t\t\t\t# loop thru all the data\n",
    "\t\t\t\t\tfor j in range(f_i.shape[0]):\n",
    "\t\t\t\t\t\tf_i_ = f_i[j]\n",
    "\t\t\t\t\t\tprediction = np.dot(self.w, f_i_) + bias \n",
    "\t\t\t\t\t\tpredictions[j] = prediction\n",
    "\n",
    "\t\t\t\t\t\t# check if this is a mis-classification case\n",
    "\t\t\t\t\t\tif(1 - y[j] * (np.dot(self.w, f_i_) + bias) > 0): # misclassification\n",
    "\t\t\t\t\t\t\t# loop thru the elements in the weight vector\n",
    "\t\t\t\t\t\t\tfor i_ in range(self.w.shape[0]):\n",
    "\t\t\t\t\t\t\t\tself.w[i_] = self.w[i_] - alpha * (-y[j] * f_i_[i_] + l * self.w[i_])\n",
    "\n",
    "\t\t\t\t\t\t\tbias = alpha - y[j] * alpha\n",
    "\n",
    "\t\t\t\t\t\telse: # no misclassification occured\n",
    "\t\t\t\t\t\t\t# loop thru the elements of the weight vector again \n",
    "\t\t\t\t\t\t\tfor i_ in range(self.w.shape[0]):\n",
    "\t\t\t\t\t\t\t\tself.w[i_] = self.w[i_] - alpha * (l*self.w[i_])\n",
    "\n",
    "\t\t\t\t\tmse = loss(predictions, y)\n",
    "\n",
    "\t\t\t\t\tif(mse > previous_loss and i != 0):\n",
    "\t\t\t\t\t\tbreak\n",
    "\n",
    "\t\t\t\t\t# if the reduction in loss barely matters\n",
    "\t\t\t\t\t# we break the process\n",
    "\t\t\t\t\tif(previous_loss - mse < 1e-8 and i > 1000):\n",
    "\t\t\t\t\t\tbreak\n",
    "\n",
    "\t\t\t\t\tprevious_loss = mse\n",
    "\t\t\t\t\tprint(\"[INFO] Epoch : \" + str(i+1) + \" | Loss = \" + str(mse))\n",
    "\n",
    "\tdef predict(self, x):\n",
    "\t\tpredictions = []\n",
    "\n",
    "\t\t# for each of the new data\n",
    "\t\tfor i in range(x.shape[0]):\n",
    "\t\t\t# loop thru the training dataset\n",
    "\t\t\tprediction = 0\n",
    "\t\t\tfor j in range(self.x.shape[0]):\n",
    "\t\t\t\tprediction += self.y[j] * self.w[j] * self.kernel(x[i], self.x[j])\n",
    "\n",
    "\t\t\tprint(prediction)\n",
    "\t\t\tif(prediction > 0):\n",
    "\t\t\t\tpredictions.append(1)\n",
    "\t\t\telse:\n",
    "\t\t\t\tpredictions.append(-1)\n",
    "\n",
    "\t\treturn predictions \n",
    "\n",
    "\n",
    "svm = KernelSVM()\n",
    "svm.fit(x,y)\n",
    "\n",
    "predictions = svm.predict(x_)\n",
    "print(predictions)\n",
    "# time to make prediction to see if it is true\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
